{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baiet\\Desktop\\NLP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baiet\\Desktop\\NLP\\.venv\\Lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for eriktks/conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/eriktks/conll2003\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def filter_null_rows(example):\n",
    "    '''Checking and removing examples with None values in 'text' or 'label'.'''\n",
    "    return example['text'] is not None and example['label'] is not None\n",
    "\n",
    "# Preparing the dataset\n",
    "dataset_path = \"eriktks/conll2003\"\n",
    "dataset = load_dataset(dataset_path).remove_columns(['id', 'pos_tags', 'chunk_tags'])\n",
    "num_labels = len(dataset[\"train\"].features[\"ner_tags\"].feature.names)\n",
    "\n",
    "# Split into train, validation and test\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_Tok_Clas(model_name, dataset_path, train, val):\n",
    "\n",
    "    def set_seed(seed):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    " \n",
    "    set_seed(123)# Set the seed to ensure reproducibility\n",
    "\n",
    "    # Load the pre-trained tokenizer and model for Token classification\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    \n",
    "    def tokenize_and_align_labels(examples):\n",
    "        '''Function to tokenize and align labels for token classification'''\n",
    "        tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True,padding='max_length', is_split_into_words=True, max_length= 100)\n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            label_ids = [-100 if word_id is None else label[word_id] for word_id in word_ids]\n",
    "            labels.append(label_ids)\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "\n",
    "    tok_train = train.map(tokenize_and_align_labels, batched=True)\n",
    "    tok_val = val.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "    # Define training arguments for the Trainer\n",
    "    training_args = TrainingArguments(\n",
    "        seed=123,\n",
    "        data_seed=123,\n",
    "        output_dir=f\"./results_{model_name.split('/')[1]}_{dataset_path.split('/')[1]}\", # Output directory for results\n",
    "        evaluation_strategy='epoch',  # Evaluate the model at the end of each epoch\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01)  # Weight decay for regularization\n",
    "    \n",
    "    # Initialize the Trainer with the model, training arguments, and datasets\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tok_train,\n",
    "        eval_dataset=tok_val)\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the trained model and tokenizer to the specified directory\n",
    "    model.save_pretrained(f\"./{model_name.split('/')[1]}_{dataset_path.split('/')[1]}\")\n",
    "    tokenizer.save_pretrained(f\"./{model_name.split('/')[1]}_{dataset_path.split('/')[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baiet\\Desktop\\NLP\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of SqueezeBertForTokenClassification were not initialized from the model checkpoint at squeezebert/squeezebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\baiet\\Desktop\\NLP\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 19%|â–ˆâ–‰        | 500/2634 [01:04<04:31,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7718, 'grad_norm': 1.0940091609954834, 'learning_rate': 1.6203492786636296e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 877/2634 [01:53<03:45,  7.79it/s]\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 879/2634 [02:00<49:40,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26540929079055786, 'eval_runtime': 6.824, 'eval_samples_per_second': 476.262, 'eval_steps_per_second': 29.895, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1000/2634 [02:16<03:25,  7.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3335, 'grad_norm': 1.6419051885604858, 'learning_rate': 1.240698557327259e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1500/2634 [03:20<02:24,  7.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2249, 'grad_norm': 2.707181215286255, 'learning_rate': 8.610478359908885e-06, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1755/2634 [03:54<01:49,  7.99it/s]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1757/2634 [04:00<24:15,  1.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16735686361789703, 'eval_runtime': 6.6649, 'eval_samples_per_second': 487.627, 'eval_steps_per_second': 30.608, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2000/2634 [04:31<01:18,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1803, 'grad_norm': 0.8626317977905273, 'learning_rate': 4.8139711465451785e-06, 'epoch': 2.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2500/2634 [05:35<00:16,  7.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1577, 'grad_norm': 1.0971603393554688, 'learning_rate': 1.0174639331814731e-06, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2634/2634 [05:59<00:00,  7.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14918379485607147, 'eval_runtime': 6.6378, 'eval_samples_per_second': 489.618, 'eval_steps_per_second': 30.733, 'epoch': 3.0}\n",
      "{'train_runtime': 359.4554, 'train_samples_per_second': 117.186, 'train_steps_per_second': 7.328, 'train_loss': 0.32506431660691987, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "Training_Tok_Clas(\"squeezebert/squeezebert-uncased\", \"eriktks/conll2003\", train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14041/14041 [00:03<00:00, 4423.30 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3250/3250 [00:00<00:00, 3740.93 examples/s]\n",
      "c:\\Users\\baiet\\Desktop\\NLP\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/2634 [00:00<?, ?it/s]c:\\Users\\baiet\\Desktop\\NLP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      " 19%|â–ˆâ–‰        | 500/2634 [01:13<05:13,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2305, 'grad_norm': 2.7367660999298096, 'learning_rate': 1.6203492786636296e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 878/2634 [02:12<03:55,  7.47it/s]\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 879/2634 [02:21<1:22:49,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06300098448991776, 'eval_runtime': 8.9823, 'eval_samples_per_second': 361.821, 'eval_steps_per_second': 22.711, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 1000/2634 [02:39<03:57,  6.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0763, 'grad_norm': 1.6752288341522217, 'learning_rate': 1.240698557327259e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1500/2634 [04:01<02:46,  6.82it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0461, 'grad_norm': 5.283290386199951, 'learning_rate': 8.610478359908885e-06, 'epoch': 1.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1756/2634 [04:42<01:56,  7.55it/s]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 1757/2634 [04:51<41:20,  2.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05373615771532059, 'eval_runtime': 8.9761, 'eval_samples_per_second': 362.073, 'eval_steps_per_second': 22.727, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 2000/2634 [05:27<01:32,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.033, 'grad_norm': 1.7722903490066528, 'learning_rate': 4.8139711465451785e-06, 'epoch': 2.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2500/2634 [06:48<00:19,  6.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0258, 'grad_norm': 0.18871574103832245, 'learning_rate': 1.0174639331814731e-06, 'epoch': 2.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2634/2634 [07:16<00:00,  7.50it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2634/2634 [07:25<00:00,  5.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.05526485666632652, 'eval_runtime': 8.978, 'eval_samples_per_second': 361.994, 'eval_steps_per_second': 22.722, 'epoch': 3.0}\n",
      "{'train_runtime': 445.4917, 'train_samples_per_second': 94.554, 'train_steps_per_second': 5.913, 'train_loss': 0.07933414371609235, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "Training_Tok_Clas(\"google-bert/bert-base-uncased\", \"eriktks/conll2003\", train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Testing_Tok_Clas(model_name, dataset_path, test):\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(f\"./{model_name.split('/')[1]}_{dataset_path.split('/')[1]}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f\"./{model_name.split('/')[1]}_{dataset_path.split('/')[1]}\")\n",
    "\n",
    "    def set_seed(seed):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    set_seed(123)# Set the seed to ensure reproducibility\n",
    "    \n",
    "    def tokenize_and_align_labels(examples):\n",
    "        tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True,padding='max_length', is_split_into_words=True, max_length= 100)\n",
    "        labels = []\n",
    "        for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "            word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "            label_ids = [-100 if word_id is None else label[word_id] for word_id in word_ids]\n",
    "            labels.append(label_ids)\n",
    "        tokenized_inputs[\"labels\"] = labels\n",
    "        return tokenized_inputs\n",
    "\n",
    "    tok_test = test.map(tokenize_and_align_labels, batched=True)\n",
    "    label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "\n",
    "    metric = load_metric(\"seqeval\")\n",
    "\n",
    "    def compute_metrics(eval_preds):\n",
    "        '''Cmputing evaluation metrics'''\n",
    "    \n",
    "        pred_logits, labels = eval_preds \n",
    "        \n",
    "        pred_logits = np.argmax(pred_logits, axis=2) \n",
    "\n",
    "        predictions = [ \n",
    "            [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100] \n",
    "            for prediction, label in zip(pred_logits, labels)] \n",
    "        \n",
    "        true_labels = [ \n",
    "        [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100] \n",
    "        for prediction, label in zip(pred_logits, labels)] \n",
    "        results = metric.compute(predictions=predictions, references=true_labels) \n",
    "        return { \n",
    "            \"precision\": results[\"overall_precision\"], \n",
    "            \"recall\": results[\"overall_recall\"], \n",
    "            \"f1\": results[\"overall_f1\"], \n",
    "            \"accuracy\": results[\"overall_accuracy\"]}\n",
    "    \n",
    "    # Define training arguments for the Trainer\n",
    "    testing_args = TrainingArguments(\n",
    "        seed=123,\n",
    "        data_seed=123,\n",
    "        output_dir=\"./results\", # Output directory for results\n",
    "        use_cpu=True) # Use CPU for testing (set to False if GPU is available)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        args=testing_args,\n",
    "        model=model,\n",
    "        eval_dataset=tok_test,\n",
    "        compute_metrics=compute_metrics)\n",
    "\n",
    "    return trainer.predict(tok_test)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baiet\\Desktop\\NLP\\.venv\\Lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 432/432 [02:52<00:00,  2.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.17675328254699707,\n",
       " 'test_precision': 0.8467713787085515,\n",
       " 'test_recall': 0.8655012486621477,\n",
       " 'test_f1': 0.8560338743824982,\n",
       " 'test_accuracy': 0.9674151670728967,\n",
       " 'test_runtime': 172.9217,\n",
       " 'test_samples_per_second': 19.969,\n",
       " 'test_steps_per_second': 2.498}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Testing_Tok_Clas(\"squeezebert/squeezebert-uncased\", \"eriktks/conll2003\", test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3453/3453 [00:00<00:00, 4689.11 examples/s]\n",
      "c:\\Users\\baiet\\Desktop\\NLP\\.venv\\Lib\\site-packages\\datasets\\load.py:759: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 432/432 [04:59<00:00,  1.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.12570184469223022,\n",
       " 'test_precision': 0.8886158886158886,\n",
       " 'test_recall': 0.9031989535022,\n",
       " 'test_f1': 0.8958480773767398,\n",
       " 'test_accuracy': 0.9756261992428564,\n",
       " 'test_runtime': 300.034,\n",
       " 'test_samples_per_second': 11.509,\n",
       " 'test_steps_per_second': 1.44}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Testing_Tok_Clas(\"google-bert/bert-base-uncased\", \"eriktks/conll2003\", test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
