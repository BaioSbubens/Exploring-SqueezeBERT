{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baiet\\Desktop\\NLP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34868/34868 [00:00<00:00, 445654.87 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3228/3228 [00:00<00:00, 206579.39 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3096/3096 [00:00<00:00, 229894.05 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def filter_null_rows(example):\n",
    "    '''Checking and removing examples with None values in 'text' or 'label'.'''\n",
    "    return example['text'] is not None \n",
    "\n",
    "# Preparing the dataset\n",
    "dataset_path = \"benjaminbeilharz/better_daily_dialog\"\n",
    "dataset = load_dataset(dataset_path, split={'train': 'train[:40%]', 'validation': 'validation[:40%]', 'test': 'test[:40%]'})\n",
    "dataset = dataset.remove_columns(['dialog_id', 'turn_type', 'emotion']).rename_column(\"utterance\", \"text\").filter(filter_null_rows)\n",
    "\n",
    "# Split into train, validation and test\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_MaskedLM(model_name, dataset_path, train, val):\n",
    "\n",
    "    def set_seed(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    set_seed(123)# Set the seed to ensure reproducibility\n",
    "\n",
    "    # Load the pre-trained tokenizer and model for sequence classification\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    \n",
    "    def tokenizer_function(examples):\n",
    "        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=50)\n",
    "\n",
    "    tok_train = train.map(tokenizer_function, batched=True)\n",
    "    tok_val = val.map(tokenizer_function, batched=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        seed=123,\n",
    "        data_seed=123,\n",
    "        output_dir=f\"./results_{model_name.split('/')[1]}_{dataset_path.split('/')[1]}\", # Output directory for results\n",
    "        evaluation_strategy='epoch', # Evaluate the model at the end of each epoch\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01) # Weight decay for regularization\n",
    "    \n",
    "    # This data collator will be used during training to dynamically mask \n",
    "    # tokens in the input text for the language modeling task.\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,  # The tokenizer used for processing the text data\n",
    "        mlm=True,  # Enable masked language modeling (MLM)\n",
    "        mlm_probability=0.1)  # Probability of masking tokens in the input text\n",
    "    \n",
    "    # Initialize the Trainer with the model, training arguments, and datasets\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tok_train,\n",
    "        eval_dataset=tok_val,\n",
    "        data_collator=data_collator)\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "    # Saving the trained model and tokenizer to the specified directory\n",
    "    model.save_pretrained(f\"./{model_name.split('/')[1]}_{dataset_path.split('/')[1]}\")\n",
    "    tokenizer.save_pretrained(f\"./{model_name.split('/')[1]}_{dataset_path.split('/')[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baiet\\Desktop\\NLP\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34868/34868 [00:02<00:00, 14960.14 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3228/3228 [00:00<00:00, 14220.14 examples/s]\n",
      "c:\\Users\\baiet\\Desktop\\NLP\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 11%|â–ˆâ–        | 500/4360 [00:55<06:36,  9.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8954, 'grad_norm': 12.67947006225586, 'learning_rate': 1.7706422018348625e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–Ž       | 1000/4360 [01:49<06:00,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8514, 'grad_norm': 13.97727108001709, 'learning_rate': 1.541284403669725e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 1500/4360 [02:48<04:58,  9.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8193, 'grad_norm': 16.585166931152344, 'learning_rate': 1.3119266055045871e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2000/4360 [03:42<04:11,  9.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7916, 'grad_norm': 14.549799919128418, 'learning_rate': 1.0825688073394496e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2179/4360 [04:02<03:50,  9.47it/s]c:\\Users\\baiet\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:306: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n",
      "                                                   \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2181/4360 [04:09<47:32,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8743982315063477, 'eval_runtime': 6.2848, 'eval_samples_per_second': 513.624, 'eval_steps_per_second': 32.141, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2500/4360 [04:43<03:13,  9.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7934, 'grad_norm': 14.174108505249023, 'learning_rate': 8.53211009174312e-06, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3000/4360 [05:37<02:23,  9.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.746, 'grad_norm': 24.039159774780273, 'learning_rate': 6.238532110091744e-06, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3500/4360 [06:32<01:39,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7379, 'grad_norm': 15.136982917785645, 'learning_rate': 3.944954128440367e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4000/4360 [07:27<00:38,  9.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7277, 'grad_norm': 15.463316917419434, 'learning_rate': 1.6513761467889911e-06, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4360/4360 [08:12<00:00,  8.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8099689483642578, 'eval_runtime': 5.8849, 'eval_samples_per_second': 548.519, 'eval_steps_per_second': 34.325, 'epoch': 2.0}\n",
      "{'train_runtime': 492.5045, 'train_samples_per_second': 141.595, 'train_steps_per_second': 8.853, 'train_loss': 1.7881281301515912, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "Training_MaskedLM(\"squeezebert/squeezebert-uncased\", \"benjaminbeilharz/better_daily_dialog\", train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34868/34868 [00:02<00:00, 15699.60 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3228/3228 [00:00<00:00, 14632.81 examples/s]\n",
      "c:\\Users\\baiet\\Desktop\\NLP\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/4360 [00:00<?, ?it/s]c:\\Users\\baiet\\Desktop\\NLP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      " 11%|â–ˆâ–        | 500/4360 [00:54<06:53,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7819, 'grad_norm': 17.237581253051758, 'learning_rate': 1.7706422018348625e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–Ž       | 1000/4360 [01:50<06:07,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6758, 'grad_norm': 21.197031021118164, 'learning_rate': 1.541284403669725e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 1500/4360 [02:46<05:11,  9.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.617, 'grad_norm': 22.381107330322266, 'learning_rate': 1.3119266055045871e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 2000/4360 [03:41<04:18,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5747, 'grad_norm': 23.087764739990234, 'learning_rate': 1.0825688073394496e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 2179/4360 [04:02<03:57,  9.17it/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2181/4360 [04:09<58:07,  1.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.7305535078048706, 'eval_runtime': 6.4957, 'eval_samples_per_second': 496.943, 'eval_steps_per_second': 31.097, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 2500/4360 [04:44<03:20,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5658, 'grad_norm': 17.855167388916016, 'learning_rate': 8.53211009174312e-06, 'epoch': 1.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 3000/4360 [05:39<02:28,  9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5038, 'grad_norm': 34.42122268676758, 'learning_rate': 6.238532110091744e-06, 'epoch': 1.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 3500/4360 [06:35<01:33,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.497, 'grad_norm': 16.468236923217773, 'learning_rate': 3.944954128440367e-06, 'epoch': 1.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 4000/4360 [07:31<00:39,  9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5248, 'grad_norm': 18.569360733032227, 'learning_rate': 1.6513761467889911e-06, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 4359/4360 [08:12<00:00,  9.16it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4360/4360 [08:19<00:00,  8.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5895793437957764, 'eval_runtime': 6.516, 'eval_samples_per_second': 495.392, 'eval_steps_per_second': 31.0, 'epoch': 2.0}\n",
      "{'train_runtime': 499.3428, 'train_samples_per_second': 139.656, 'train_steps_per_second': 8.731, 'train_loss': 1.5835528872428684, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "Training_MaskedLM(\"google-bert/bert-base-uncased\", \"benjaminbeilharz/better_daily_dialog\", train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Testing_MaskedLM(model_name, dataset_path, test_dataset):\n",
    "\n",
    "    def set_seed(seed):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    set_seed(123)# Set the seed to ensure reproducibility\n",
    "    \n",
    "    # Load the trained model and tokenizer\n",
    "    model = AutoModelForMaskedLM.from_pretrained(f\"./{model_name.split('/')[1]}_{dataset_path.split('/')[1]}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f\"./{model_name.split('/')[1]}_{dataset_path.split('/')[1]}\")\n",
    "    \n",
    "    def tokenizer_function(examples):\n",
    "        '''Tokenizer function to preprocess the text data'''\n",
    "        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=50)\n",
    "\n",
    "    tok_test = test_dataset.map(tokenizer_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.1)\n",
    "    test_dataloader = DataLoader(tok_test, batch_size=16, collate_fn=data_collator, num_workers=4)\n",
    "\n",
    "    def cosine_similarity(emb1, emb2):\n",
    "        '''Computing cosine similarity'''\n",
    "        return torch.nn.functional.cosine_similarity(emb1, emb2, dim=-1).mean().item()\n",
    "\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    model.to('cpu') # Move the model to CPU (change to 'cuda' if GPU is available)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop over batches in the test dataloader\n",
    "    for batch in test_dataloader:\n",
    "        # Move input tensors to the same device as the model\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        labels = batch['labels'].to(model.device)\n",
    "\n",
    "        # Perform inference without tracking gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        # Get the predicted token ids\n",
    "        preds = logits.argmax(dim=-1)\n",
    "\n",
    "        # Store the predictions and true labels\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_labels.append(labels.cpu())\n",
    "\n",
    "    end_time = time.time()\n",
    "    runtime = end_time - start_time\n",
    "\n",
    "    # Concatenate all predictions and labels into single tensors\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "\n",
    "    # Create a mask to filter out padding tokens from predictions and labels\n",
    "    mask = all_labels != -100\n",
    "    filtered_preds = all_preds[mask]\n",
    "    filtered_labels = all_labels[mask]\n",
    "\n",
    "    acc = accuracy_score(filtered_labels.numpy(), filtered_preds.numpy())\n",
    "\n",
    "    # Get embeddings for the predicted and true tokens\n",
    "    pred_embeddings = model.get_input_embeddings()(filtered_preds.to(model.device))\n",
    "    label_embeddings = model.get_input_embeddings()(filtered_labels.to(model.device))\n",
    "\n",
    "    cosine_similarities = []\n",
    "    for pred_emb, label_emb in zip(pred_embeddings, label_embeddings):\n",
    "        cosine_similarities.append(cosine_similarity(pred_emb.unsqueeze(0), label_emb.unsqueeze(0)))\n",
    "\n",
    "    avg_cosine_similarity = np.mean(cosine_similarities)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": acc,\n",
    "        \"avg_cosine_similarity\": avg_cosine_similarity,\n",
    "        \"runtime_seconds\": runtime\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "#Usually, in the other tasks we call the trainer object and the .predict() method to test the model.\n",
    "#Thus, this type of method raised errors for this task, so we have implemented the testing phase from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3096/3096 [00:00<00:00, 15028.35 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6184945275854367,\n",
       " 'avg_cosine_similarity': 0.6971506856962072,\n",
       " 'runtime_seconds': 115.05589461326599}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Testing_MaskedLM(\"squeezebert/squeezebert-uncased\",\"benjaminbeilharz/better_daily_dialog\", test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3096/3096 [00:00<00:00, 16463.01 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6625486158773736,\n",
       " 'avg_cosine_similarity': 0.7819762470013584,\n",
       " 'runtime_seconds': 174.75553488731384}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Testing_MaskedLM(\"google-bert/bert-base-uncased\",\"benjaminbeilharz/better_daily_dialog\", test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
