{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_null_rows(example):\n",
    "    '''It checks if there is some None examples in the data, and remove it'''\n",
    "    return example['text'] is not None and example['label'] is not None\n",
    "\n",
    "def map_labels(example):\n",
    "    label_map = {label: i for i, label in enumerate(set(dataset['label']))}\n",
    "    example['label'] = label_map[example['label']]\n",
    "    return example\n",
    "\n",
    "dataset_path = \"valurank/News_Articles_Categorization\"\n",
    "\n",
    "dataset = load_dataset(dataset_path)['train'].rename_column(\"Text\", \"text\").rename_column(\"Category\", \"label\")\n",
    "dataset = dataset.filter(filter_null_rows).map(map_labels)\n",
    "\n",
    "num_labels = len(np.unique(dataset['label']))\n",
    "\n",
    "train_val_test_split = dataset.train_test_split(test_size=0.2, seed=123)\n",
    "train_dataset = train_val_test_split['train']\n",
    "temp_dataset = train_val_test_split['test']\n",
    "\n",
    "\n",
    "val_test_split = temp_dataset.train_test_split(test_size=0.5, seed=123)\n",
    "val_dataset = val_test_split['train']\n",
    "test_dataset = val_test_split['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Training_Seq_Clas(model_name, dataset_path, train, val):\n",
    "\n",
    "    def set_seed(seed):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    set_seed(123)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "    \n",
    "    def tokenizer_function(examples):\n",
    "        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=300)\n",
    "\n",
    "    tok_train = train.map(tokenizer_function, batched=True)\n",
    "    tok_val = val.map(tokenizer_function, batched=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        seed=123,\n",
    "        data_seed=123,\n",
    "        output_dir=f\"./results_{model_name.split('/')[1]}_{dataset_path.split('/')[1]}\",\n",
    "        evaluation_strategy='epoch',\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tok_train,\n",
    "        eval_dataset=tok_val)\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "    model.save_pretrained(f\"./{model_name.split('/')[1]}_{dataset_path.split('/')[1]}\")\n",
    "    tokenizer.save_pretrained(f\"./{model_name.split('/')[1]}_{dataset_path.split('/')[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baiet\\Desktop\\NLP\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of SqueezeBertForSequenceClassification were not initialized from the model checkpoint at squeezebert/squeezebert-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\baiet\\Desktop\\NLP\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 187/561 [01:11<02:02,  3.05it/s]\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 187/561 [01:14<02:02,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7639732956886292, 'eval_runtime': 2.2966, 'eval_samples_per_second': 161.981, 'eval_steps_per_second': 10.45, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 374/561 [02:27<00:56,  3.34it/s]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 374/561 [02:29<00:56,  3.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43972089886665344, 'eval_runtime': 2.3062, 'eval_samples_per_second': 161.304, 'eval_steps_per_second': 10.407, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 500/561 [03:15<00:22,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.812, 'grad_norm': 3.6935312747955322, 'learning_rate': 2.1746880570409984e-06, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 561/561 [03:38<00:00,  3.44it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 561/561 [03:40<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3686646521091461, 'eval_runtime': 2.215, 'eval_samples_per_second': 167.943, 'eval_steps_per_second': 10.835, 'epoch': 3.0}\n",
      "{'train_runtime': 220.5584, 'train_samples_per_second': 40.493, 'train_steps_per_second': 2.544, 'train_loss': 0.7600495615532479, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "Training_Seq_Clas(\"squeezebert/squeezebert-uncased\", \"valurank/News_Articles_Categorization\", train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2977/2977 [00:04<00:00, 735.49 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 372/372 [00:00<00:00, 677.93 examples/s]\n",
      "c:\\Users\\baiet\\Desktop\\NLP\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 0/561 [00:00<?, ?it/s]c:\\Users\\baiet\\Desktop\\NLP\\.venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "                                                 \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 187/561 [01:17<01:55,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3143351376056671, 'eval_runtime': 2.844, 'eval_samples_per_second': 130.803, 'eval_steps_per_second': 8.439, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 374/561 [02:34<00:58,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21037505567073822, 'eval_runtime': 2.8678, 'eval_samples_per_second': 129.717, 'eval_steps_per_second': 8.369, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 500/561 [03:25<00:24,  2.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4318, 'grad_norm': 0.5544067025184631, 'learning_rate': 2.1746880570409984e-06, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 561/561 [03:55<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.17613372206687927, 'eval_runtime': 2.861, 'eval_samples_per_second': 130.026, 'eval_steps_per_second': 8.389, 'epoch': 3.0}\n",
      "{'train_runtime': 235.0752, 'train_samples_per_second': 37.992, 'train_steps_per_second': 2.386, 'train_loss': 0.39512487401299295, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "Training_Seq_Clas(\"google-bert/bert-base-uncased\", \"valurank/News_Articles_Categorization\", train_dataset, val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Testing_Seq_Clas(model_name, dataset_path, test):\n",
    "    np.random.seed(123)\n",
    "    torch.manual_seed(123)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(123)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(f\"./{model_name.split('/')[1]}_{dataset_path.split('/')[1]}\", num_labels=num_labels)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(f\"./{model_name.split('/')[1]}_{dataset_path.split('/')[1]}\")\n",
    "    \n",
    "    def tokenizer_function(examples):\n",
    "        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=300)\n",
    "\n",
    "    tok_test = test.map(tokenizer_function, batched=True)\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        \n",
    "        accuracy_metric = load_metric(\"accuracy\", trust_remote_code=True)\n",
    "        f1_metric = load_metric(\"f1\", trust_remote_code=True)\n",
    "        precision_metric = load_metric(\"precision\", trust_remote_code=True)\n",
    "        recall_metric = load_metric(\"recall\", trust_remote_code=True)\n",
    "        \n",
    "        accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "        f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "        precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "        recall = recall_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": accuracy[\"accuracy\"],\n",
    "            \"f1\": f1[\"f1\"],\n",
    "            \"precision\": precision[\"precision\"],\n",
    "            \"recall\": recall[\"recall\"]}\n",
    "    \n",
    "    testing_args = TrainingArguments(\n",
    "        seed=123,\n",
    "        data_seed=123,\n",
    "        output_dir=\"./results\",\n",
    "        use_cpu=True)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        args=testing_args,\n",
    "        model=model,\n",
    "        eval_dataset=tok_test,\n",
    "        compute_metrics=compute_metrics)\n",
    "\n",
    "    return trainer.predict(tok_test)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [01:01<00:00,  1.31s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.3123633563518524,\n",
       " 'test_accuracy': 0.9463806970509383,\n",
       " 'test_f1': 0.947022070532522,\n",
       " 'test_precision': 0.9489325271094708,\n",
       " 'test_recall': 0.9463806970509383,\n",
       " 'test_runtime': 62.6655,\n",
       " 'test_samples_per_second': 5.952,\n",
       " 'test_steps_per_second': 0.75}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Testing_Seq_Clas(\"squeezebert/squeezebert-uncased\", \"valurank/News_Articles_Categorization\", test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [01:37<00:00,  2.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.12301452457904816,\n",
       " 'test_accuracy': 0.9705093833780161,\n",
       " 'test_f1': 0.9704613911860033,\n",
       " 'test_precision': 0.9713580432079093,\n",
       " 'test_recall': 0.9705093833780161,\n",
       " 'test_runtime': 99.3161,\n",
       " 'test_samples_per_second': 3.756,\n",
       " 'test_steps_per_second': 0.473}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Testing_Seq_Clas(\"google-bert/bert-base-uncased\", \"valurank/News_Articles_Categorization\", test_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
